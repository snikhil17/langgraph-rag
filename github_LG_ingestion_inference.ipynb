{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pgvector psycopg2-binary gitpython -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "\n",
    "REPO_URL  = \"https://github.com/langchain-ai/langgraph.git\"\n",
    "LOCAL_DIR = Path(\"LG-python\")\n",
    "\n",
    "if LOCAL_DIR.exists():\n",
    "    repo = Repo(str(LOCAL_DIR))\n",
    "    repo.remote().pull()          # fast refresh\n",
    "else:\n",
    "    Repo.clone_from(REPO_URL, LOCAL_DIR, depth=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 (replace)\n",
    "import ast, textwrap, hashlib, inspect\n",
    "\n",
    "def iter_code_chunks(py_path: Path):\n",
    "    src = py_path.read_text(encoding=\"utf-8\")\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError:\n",
    "        return\n",
    "\n",
    "    # 1️⃣  top-level import block (file header)\n",
    "    import_lines = [n for n in tree.body if isinstance(n, (ast.Import, ast.ImportFrom))]\n",
    "    if import_lines:\n",
    "        start = import_lines[0].lineno - 1\n",
    "        end   = import_lines[-1].end_lineno\n",
    "        block = \"\\n\".join(src.splitlines()[start:end])\n",
    "        uid   = hashlib.sha256(f\"{py_path}:imports\".encode()).hexdigest()\n",
    "        yield {\n",
    "            \"id\": uid,\n",
    "            \"content\": block,\n",
    "            \"metadata\": {\"path\": str(py_path.relative_to(LOCAL_DIR)),\n",
    "                         \"chunk_id\": -1,\n",
    "                         \"source\": \"import_block\"}\n",
    "        }\n",
    "\n",
    "    # 2️⃣  functions / classes with docstring merged\n",
    "    for idx, node in enumerate(tree.body):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "            start = node.lineno - 1\n",
    "            end   = getattr(node, \"end_lineno\", start + 1)\n",
    "            lines = src.splitlines()\n",
    "            body  = \"\\n\".join(lines[start:end])\n",
    "\n",
    "            # pull the docstring (if any) and prepend\n",
    "            doc  = ast.get_docstring(node)\n",
    "            if doc:\n",
    "                body = f'\"\"\"{doc}\"\"\"\\n' + body.replace(f'\"\"\"{doc}\"\"\"', \"\", 1)\n",
    "\n",
    "            uid  = hashlib.sha256(f\"{py_path}:{idx}\".encode()).hexdigest()\n",
    "            yield {\n",
    "                \"id\": uid,\n",
    "                \"content\": textwrap.dedent(body),\n",
    "                \"metadata\": {\"path\": str(py_path.relative_to(LOCAL_DIR)),\n",
    "                             \"chunk_id\": idx,\n",
    "                             \"source\": \"func\" if isinstance(node, ast.FunctionDef) else \"class\"}\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = Path(\"contextual_chunked_LG_docs/raw_chunks\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from langchain_core.documents import Document                 # LangChain doc\n",
    "contextual_chunks = []\n",
    "\n",
    "for py in LOCAL_DIR.rglob(\"*.py\"):\n",
    "    for c in iter_code_chunks(py):\n",
    "        # 3a. write each chunk as a .txt file for transparency\n",
    "        safe_path = c[\"metadata\"][\"path\"].replace(\"/\", \"__\")\n",
    "        fname = RAW_DIR / f\"{safe_path}__{c['metadata']['chunk_id']}.txt\"\n",
    "        fname.write_text(c[\"content\"], encoding=\"utf-8\")\n",
    "        # 3b. build Document\n",
    "        contextual_chunks.append(Document(page_content=c[\"content\"],\n",
    "                                          metadata=c[\"metadata\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761477/3190965533.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name = '/mnt/c/Users/Nikhil/Desktop/chai_code_genai/RAG_on_chaicode_docs_website/models/all-mpnet-base-v2',\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = '/mnt/c/Users/Nikhil/Desktop/chai_code_genai/RAG_on_chaicode_docs_website/models/all-mpnet-base-v2',\n",
    "                                        model_kwargs={'device': 'cuda'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chroma collection adk_code persisted in contextual_chunked_LG_docs\n"
     ]
    }
   ],
   "source": [
    "                                                               # 384-d embs\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=contextual_chunks,\n",
    "    embedding=embedding_model,                                     # Embeddings obj\n",
    "    collection_name=\"lg_code\",\n",
    "    persist_directory=\"contextual_chunked_LG_docs/github_lg\"\n",
    ")\n",
    "vectorstore.persist()\n",
    "print(\"✓ Chroma collection adk_code persisted in contextual_chunked_LG_docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs/sdk-py/langgraph_sdk/schema.py → \"\"\"Base model for an assistant.\"\"\" …\n",
      "libs/langgraph/bench/wide_dict.py → def wide_dict(n: int) -> StateGraph: …\n",
      "libs/langgraph/bench/wide_state.py → def wide_state(n: int) -> StateGraph: …\n",
      "libs/sdk-py/langgraph_sdk/schema.py → \"\"\"Represents a conversation thread.\"\"\" …\n",
      "libs/sdk-py/langgraph_sdk/schema.py → \"\"\"Defines the structure and properties of a graph.\"\"\" …\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(\n",
    "    \"How to strore interactions in Database ?\", k=5)\n",
    "for d in docs:\n",
    "    print(d.metadata[\"path\"], \"→\", d.page_content.splitlines()[0][:80], \"…\")\n",
    "    # print(d.metadata[\"path\"], \"→\", d.page_content, \"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = HuggingFaceEmbeddings(model_name = '/mnt/c/Users/Nikhil/Desktop/chai_code_genai/RAG_on_chaicode_docs_website/models/all-mpnet-base-v2',\n",
    "#                                         model_kwargs={'device': 'cuda'})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# \n",
    "# \n",
    "gemini_llm  = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq  \n",
    "llm   = ChatGroq(model=\"meta-llama/llama-4-maverick-17b-128e-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f13ea54ab10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 0. Imports ------------------------------------------------------------\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "\n",
    "from langchain_community.vectorstores import Chroma                        # :contentReference[oaicite:4]{index=4}\n",
    "from langchain_huggingface import HuggingFaceEmbeddings                    # :contentReference[oaicite:5]{index=5}\n",
    "from langchain.retrievers import EnsembleRetriever                         # :contentReference[oaicite:6]{index=6}\n",
    "from langchain.prompts import PromptTemplate\n",
    "                                       # :contentReference[oaicite:7]{index=7}\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 1. Shared state -------------------------------------------------------\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    context_docs: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# --- 2. Global resources ---------------------------------------------------\n",
    "# a) Embeddings (same model for both stores)\n",
    "\n",
    "\n",
    "# b) Chroma vector stores\n",
    "# docs_vs  = Chroma(\n",
    "#     collection_name=\"lg_docs\",\n",
    "#     persist_directory=\"contextual_chunked_lg_docs\",\n",
    "#     embedding_function=embedding_model,\n",
    "# )\n",
    "code_vs  = Chroma(\n",
    "    collection_name=\"lg_code\",\n",
    "    persist_directory=\"contextual_chunked_lg_docs/github_lg\",  # same folder OK\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "# c) Ensemble retriever\n",
    "# docs_ret = docs_vs.as_retriever(search_kwargs={\"k\": 6})\n",
    "code_ret = code_vs.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ensemble_ret = EnsembleRetriever(\n",
    "#     retrievers=[docs_ret, code_ret],\n",
    "#     weights=[0.4, 0.6]             # tune after offline eval\n",
    "# )\n",
    "\n",
    "# --- 3. Node functions -----------------------------------------------------\n",
    "def retrieval_node(state: GraphState):\n",
    "    # docs = ensemble_ret.get_relevant_documents(state[\"question\"])\n",
    "    docs = code_vs.max_marginal_relevance_search(state[\"question\"], k=5, fetch_k=10)\n",
    "    return {\"context_docs\": docs}\n",
    "\n",
    "def llm_answer_node(state: GraphState):\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{d.metadata.get('path')}] {d.page_content}\" for d in state[\"context_docs\"]\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=(\n",
    "            # \"You are an Langgraph expert. Answer the question using ONLY the context. But keep in mind the context are like legos, you should create and craft the solutions based on context and own python knowledge \"\n",
    "            # \"Context will provide you the github repository code but you should respond with codes from the prespective of user. For e.g. context will have core classes of how pandas is written but you should provide codes as to how developer will use pandas `from pandas import pd` and other functionalities\"\n",
    "            # \"Quote file paths in brackets when you cite.\\n\\n\"\n",
    "            # \"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "            # \"Question: {question}\\nAnswer:\"\n",
    "            \"\"\" \n",
    "                You are an expert Python developer and LangGraph specialist, with deep knowledge of large language model orchestration, graph-based workflows, and system integration.\n",
    "\n",
    "                You are provided with context containing code snippets, architecture patterns, and documentation extracted from the official LangGraph repository and guides.  \n",
    "                Your task is to **build developer-usable solutions** and **explain system behaviors clearly and professionally**, depending on the type of question.\n",
    "\n",
    "                Follow these strict rules based on the nature of the question:\n",
    "\n",
    "                ---\n",
    "\n",
    "                ## 🛠️ For Developer-Focused Coding Questions:\n",
    "\n",
    "                1. **Developer's Perspective:**\n",
    "                    - Focus on writing practical, runnable code that a normal Python developer would use.\n",
    "                    - Prefer using built-in constructs directly (`StateGraph`, `END`, node functions, `add_node`, `add_edge`, etc.).\n",
    "                    - Do **NOT** redefine or reimplement core framework elements unless explicitly asked.\n",
    "\n",
    "                2. **Solution Crafting Approach:**\n",
    "                    - Treat the provided context as \"building blocks\" and connect them logically (like graph nodes).\n",
    "                    - If minor logical gaps exist, fill them using standard Python best practices.\n",
    "                    - Prioritize immediately runnable, graph-structured code examples.\n",
    "\n",
    "                3. **Coding Style Expectations:**\n",
    "                    - Include necessary imports and modularize code (separate graph definition, node functions, etc.).\n",
    "                    - Show simple execution examples when appropriate (e.g., running a graph, initializing with input state).\n",
    "                    - Minimize explanations — focus on delivering working graph flows.\n",
    "                    - Assume reasonable defaults for common LangGraph patterns (like async support, error handling) if not fully detailed.\n",
    "\n",
    "                4. **Contextual References:**\n",
    "                    - If referring to specific files or modules, mention them clearly in brackets (e.g., `[src/langgraph/graph/state_graph.py]`).\n",
    "\n",
    "                5. **Tone and Format:**\n",
    "                    - Be concise, direct, and action-oriented.\n",
    "                    - Only explain beyond the code when absolutely necessary to understand the graph structure.\n",
    "\n",
    "                ---\n",
    "\n",
    "                ## 🧠 For System Behavior / Senior Management / Architecture Questions:\n",
    "\n",
    "                1. **Architecture and Integration Focus:**\n",
    "                    - Explain system behaviors, memory handling, node orchestration, concurrency management, and extensibility clearly and professionally.\n",
    "                    - Focus on the \"what\" and \"why\" behind LangGraph's graph-based design.\n",
    "                    - Keep explanations business-relevant and avoid unnecessary technical deep-dives unless requested.\n",
    "\n",
    "                2. **Explanation Crafting Approach:**\n",
    "                    - Base answers on the provided context; intelligently fill minor gaps using industry-standard LLM workflow knowledge.\n",
    "                    - Highlight modularity, parallelism, customization of edges, state passing, and ease of integrating LLMs or tools.\n",
    "\n",
    "                3. **System Design Style Expectations:**\n",
    "                    - Provide executive-friendly insights that enable decision-making.\n",
    "\n",
    "                4. **Contextual References:**\n",
    "                    - Mention source files explicitly in brackets when applicable (e.g., `[src/langgraph/graph/state_graph.py]`).\n",
    "\n",
    "                5. **Tone and Format:**\n",
    "                    - Be clear, confident, and precise.\n",
    "                    - Summarize key advantages, trade-offs, and integration readiness.\n",
    "                    - Always give an intuitive understanding of the topic first, then map this example with the technical jargons\n",
    "                    - Give output in the markdown format making sure that user understands it easily.\n",
    "\n",
    "                ---\n",
    "\n",
    "                <context>  \n",
    "                {context}  \n",
    "                </context>\n",
    "\n",
    "                Question: {question}  \n",
    "                Answer:\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "        ),\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    \n",
    "    # chain = prompt | gemini_llm\n",
    "    chain = prompt | llm\n",
    "\n",
    "\n",
    "    result = chain.invoke({\"context\": context, \"question\": state[\"question\"]})\n",
    "    return {\"answer\": result}\n",
    "\n",
    "# --- 4. Build the LangGraph -----------------------------------------------\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"retrieve\", retrieval_node)\n",
    "graph.add_node(\"generate\", llm_answer_node)\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.set_finish_point(\"generate\")\n",
    "\n",
    "rag_graph = graph.compile()\n",
    "\n",
    "rag_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Answer:\n",
      " Based on the provided context, LangGraph handles memory and persistence primarily through two mechanisms: the **Checkpointer** and the **Store**.\n",
      "\n",
      "1.  **Checkpointer:**\n",
      "    *   Saves the *state* of a specific thread or run.\n",
      "    *   This state is typically the return value of the previous invocation of the entrypoint function (or the value specified by `entrypoint.final.save`).\n",
      "    *   It's used for resuming interrupted workflows and accessing the output of the immediately preceding step within the same thread via the `previous` parameter [langgraph/func/__init__.py].\n",
      "    *   You configure it by passing a `checkpointer` instance (like `MemorySaver`) to the `@entrypoint` decorator [langgraph/func/__init__.py].\n",
      "\n",
      "2.  **Store:**\n",
      "    *   Provides a mechanism for \"long-term memory\" or \"cross-thread memories\" [langgraph/sdk-py/schema.py].\n",
      "    *   It is a generalized key-value store implementing the `BaseStore` interface [langgraph/config.py].\n",
      "    *   Data is stored as `Item` objects, which include a `namespace`, `key`, and `value` (a dictionary) [langgraph/sdk-py/schema.py].\n",
      "    *   You can access the store from within a node or task using the `get_store()` function, provided the graph was compiled with a store or the entrypoint was decorated with one [langgraph/config.py].\n",
      "    *   The context shows `InMemoryStore` as an example implementation [langgraph/config.py, tests/conftest.py].\n",
      "    *   The store configuration can potentially include settings for semantic search (`index`) and time-to-live (`ttl`) [cli/langgraph_cli/config.py].\n",
      "\n",
      "**How to use a Database for Storage:**\n",
      "\n",
      "The provided context indicates that the `store` parameter expects an instance of a `BaseStore` implementation. To use a database, you would need a `BaseStore` implementation that is backed by your chosen database (e.g., a `PostgreSQLStore`, `RedisStore`, etc.).\n",
      "\n",
      "While the context doesn't provide specific database-backed `BaseStore` implementations, it shows *where* you would configure such an instance:\n",
      "\n",
      "1.  **With the `@entrypoint` decorator:** Pass an instance of your database-backed store to the `store` argument.\n",
      "    ```python\n",
      "    # Assume you have a database-backed store implementation available\n",
      "    # from langgraph.store.your_db import YourDatabaseStore\n",
      "    # db_store = YourDatabaseStore(...)\n",
      "\n",
      "    from langgraph.func import entrypoint\n",
      "    from langgraph.store.memory import InMemoryStore # Using InMemoryStore as example from context\n",
      "\n",
      "    # Replace InMemoryStore() with your database-backed store instance\n",
      "    db_store = InMemoryStore() # Placeholder for YourDatabaseStore()\n",
      "\n",
      "    @entrypoint(store=db_store)\n",
      "    def my_workflow_with_db(input_data: str):\n",
      "        from langgraph.config import get_store\n",
      "        my_store = get_store()\n",
      "        # Use my_store to put/get data from the database\n",
      "        my_store.put((\"my_namespace\",), \"some_key\", {\"data\": input_data})\n",
      "        stored_item = my_store.get((\"my_namespace\",), \"some_key\")\n",
      "        return stored_item.value[\"data\"]\n",
      "\n",
      "    # Example usage (with a config including thread_id for state persistence)\n",
      "    config = {\"configurable\": {\"thread_id\": \"user123\"}}\n",
      "    # my_workflow_with_db.invoke(\"hello world\", config)\n",
      "    ```\n",
      "    [langgraph/func/__init__.py, langgraph/config.py]\n",
      "\n",
      "2.  **With `StateGraph.compile()`:** Pass an instance of your database-backed store to the `store` argument during compilation.\n",
      "    ```python\n",
      "    # Assume you have a database-backed store implementation available\n",
      "    # from langgraph.store.your_db import YourDatabaseStore\n",
      "    # db_store = YourDatabaseStore(...)\n",
      "\n",
      "    from typing_extensions import TypedDict\n",
      "    from langgraph.graph import StateGraph, START\n",
      "    from langgraph.store.memory import InMemoryStore # Using InMemoryStore as example from context\n",
      "\n",
      "    # Replace InMemoryStore() with your database-backed store instance\n",
      "    db_store = InMemoryStore() # Placeholder for YourDatabaseStore()\n",
      "\n",
      "    class AgentState(TypedDict):\n",
      "        input: str\n",
      "        output: str\n",
      "\n",
      "    def my_db_node(state: AgentState):\n",
      "        from langgraph.config import get_store\n",
      "        my_store = get_store()\n",
      "        # Use my_store to interact with the database\n",
      "        my_store.put((\"user_sessions\",), state[\"input\"], {\"last_output\": state[\"output\"]})\n",
      "        return {\"output\": state[\"output\"] + \" processed\"}\n",
      "\n",
      "    workflow = StateGraph(AgentState)\n",
      "    workflow.add_node(\"process\", my_db_node)\n",
      "    workflow.add_edge(START, \"process\")\n",
      "\n",
      "    # Compile the graph with the database-backed store instance\n",
      "    graph = workflow.compile(store=db_store)\n",
      "\n",
      "    # Example usage\n",
      "    # config = {\"configurable\": {\"thread_id\": \"sessionXYZ\"}}\n",
      "    # graph.invoke({\"input\": \"query1\", \"output\": \"result1\"}, config)\n",
      "    ```\n",
      "    [langgraph/config.py]\n",
      "\n",
      "In summary, memory is handled by checkpointing (thread state) and the store (long-term/cross-thread data). To use a database, you need a `BaseStore` implementation for your database and pass an instance of it when defining your workflow using `@entrypoint` or `StateGraph.compile()`.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run a test ---------------------------------------------------------\n",
    "# question = \"How to strore interactions using DatabaseSessionService (when i run adk web). Tell me the exact command i should run ?\"\n",
    "question = \"How is memory stored in langgraph ? how can i use DB to store the same \"\n",
    "\n",
    "response = rag_graph.invoke({\"question\": question})\n",
    "\n",
    "print(\"🧠 Answer:\\n\", response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Answer:\n",
      " Based on the provided context, the most relevant function for creating a chatbot with memory is `create_react_agent`. The context shows how this function can incorporate thread-level memory using a `checkpointer` and also mentions the use of a `store` for cross-thread memory.\n",
      "\n",
      "However, the provided context *does not* contain code examples for:\n",
      "1.  Building the structure of a *multi-agent* system (connecting multiple distinct agents). `create_react_agent` builds a single agent loop (LLM + Tools). While it mentions being usable *within* multi-agent systems, the code to *create* the multi-agent structure itself is not present.\n",
      "2.  Using a *database* specifically for the `checkpointer` or `store`. The examples provided use `MemorySaver` for the checkpointer and `InMemoryStore` for the store.\n",
      "\n",
      "Therefore, I can show you how to create a *single* agent chatbot with in-memory thread memory and demonstrate the pattern for using an in-memory store for cross-thread memory, as these are the functionalities supported by the provided code examples.\n",
      "\n",
      "Here's how you can create a single agent chatbot with in-memory thread memory using `create_react_agent` based on the context:\n",
      "\n",
      "```python\n",
      "import time\n",
      "from typing import Annotated\n",
      "\n",
      "from langchain_core.messages import BaseMessage, SystemMessage\n",
      "from langchain_core.runnables import RunnableConfig\n",
      "from langchain_core.tools import BaseTool, tool\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langgraph.checkpoint.memory import MemorySaver\n",
      "from langgraph.prebuilt import create_react_agent, InjectedStore\n",
      "from langgraph.store.base import BaseStore\n",
      "from langgraph.store.memory import InMemoryStore\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "# Define a simple tool (as shown in the context examples)\n",
      "@tool\n",
      "def check_weather(location: str) -> str:\n",
      "    '''Return the weather forecast for the specified location.'''\n",
      "    # Simulate a delay\n",
      "    time.sleep(1)\n",
      "    return f\"It's always sunny in {location}\"\n",
      "\n",
      "# Define the language model (as shown in the context examples)\n",
      "# Replace with your actual model\n",
      "model = ChatOpenAI(model=\"gpt-4o\")\n",
      "\n",
      "# Define the tools the agent can use\n",
      "tools = [check_weather]\n",
      "\n",
      "# Create a checkpointer for thread memory (using MemorySaver as shown)\n",
      "# The context mentions Checkpointer for persisting state, but only shows MemorySaver\n",
      "checkpointer = MemorySaver()\n",
      "\n",
      "# Create the single agent graph with the model, tools, and checkpointer\n",
      "# The checkpointer enables thread-level memory for the conversation\n",
      "graph = create_react_agent(\n",
      "    model,\n",
      "    tools,\n",
      "    checkpointer=checkpointer\n",
      ")\n",
      "\n",
      "# --- Example Usage with Thread Memory ---\n",
      "print(\"--- Conversation 1 (Thread 1) ---\")\n",
      "config1 = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
      "inputs1 = {\"messages\": [(\"user\", \"What's the weather in SF?\")]}\n",
      "\n",
      "# Use stream to see the intermediate steps (optional, but shown in context)\n",
      "for s in graph.stream(inputs1, config1, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "print(\"\\n--- Continuing Conversation 1 (Thread 1) ---\")\n",
      "inputs2 = {\"messages\": [(\"user\", \"Cool, so then should i go biking today?\")]}\n",
      "\n",
      "# The agent remembers the previous turn in this thread\n",
      "for s in graph.stream(inputs2, config1, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "print(\"\\n--- Conversation 2 (Thread 2) ---\")\n",
      "config2 = {\"configurable\": {\"thread_id\": \"thread-2\"}}\n",
      "inputs3 = {\"messages\": [(\"user\", \"What's the weather in London?\")]}\n",
      "\n",
      "# This is a new thread, the agent won't remember the previous conversation\n",
      "for s in graph.stream(inputs3, config2, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "# --- Example Pattern for Cross-Thread Memory using Store (In-Memory only in context) ---\n",
      "# The context provides a pattern for cross-thread memory using a `store`\n",
      "# and injecting it into the agent's nodes. Note this example uses InMemoryStore\n",
      "# as shown in the context, not a database store.\n",
      "\n",
      "class AgentState(TypedDict):\n",
      "    messages: Annotated[list[BaseMessage], add_messages]\n",
      "\n",
      "# Tool to save memory (as shown in context)\n",
      "@tool\n",
      "def save_memory(memory: str, *, config: RunnableConfig, store: Annotated[BaseStore, InjectedStore()]) -> str:\n",
      "    '''Save the given memory for the current user.'''\n",
      "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
      "    if not user_id:\n",
      "        return \"Error: User ID not provided in config.\"\n",
      "    namespace = (\"memories\", user_id)\n",
      "    # The context example uses list length for key, let's use a simple counter pattern\n",
      "    memories_count = len(list(store.search(namespace)))\n",
      "    store.put(namespace, f\"memory_{memories_count}\", {\"data\": memory})\n",
      "    return f\"Saved memory: {memory}\"\n",
      "\n",
      "# Function to prepare model inputs by adding user memories from the store (as shown in context)\n",
      "def prepare_model_inputs(state: AgentState, config: RunnableConfig, store: Annotated[BaseStore, InjectedStore()]) -> list[BaseMessage]:\n",
      "    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n",
      "    memories = []\n",
      "    if user_id:\n",
      "        namespace = (\"memories\", user_id)\n",
      "        # Search returns Checkpoint objects, extract the value\n",
      "        memories = [m.value[\"data\"] for m in store.search(namespace)]\n",
      "\n",
      "    system_msg_content = \"You are a helpful bot.\"\n",
      "    if memories:\n",
      "        system_msg_content += f\"\\nUser memories: {', '.join(memories)}\"\n",
      "\n",
      "    # Add the system message and the current messages from the state\n",
      "    return [SystemMessage(content=system_msg_content)] + state[\"messages\"]\n",
      "\n",
      "\n",
      "# Create an in-memory store (as shown in context)\n",
      "# To use a database store, you would need an implementation like SQLStore or RedisStore\n",
      "# which is not provided in this context.\n",
      "cross_thread_store = InMemoryStore()\n",
      "\n",
      "# Create a new agent with the cross-thread memory pattern\n",
      "# Note: This agent has the 'save_memory' tool and uses the 'prepare_model_inputs' prompt\n",
      "# It also uses the checkpointer for thread memory AND the store for cross-thread memory\n",
      "graph_with_cross_memory = create_react_agent(\n",
      "    model,\n",
      "    [check_weather, save_memory], # Include the save_memory tool\n",
      "    prompt=prepare_model_inputs, # Use the custom prompt\n",
      "    store=cross_thread_store, # Provide the store\n",
      "    checkpointer=MemorySaver() # Still use checkpointer for thread memory\n",
      ")\n",
      "\n",
      "print(\"\\n--- Conversation with Cross-Thread Memory (User 1, Thread 1) ---\")\n",
      "config_user1_thread1 = {\"configurable\": {\"thread_id\": \"user1-thread-1\", \"user_id\": \"user1\"}}\n",
      "inputs_user1_1 = {\"messages\": [(\"user\", \"Hey I'm Alice, how's it going?\")]}\n",
      "\n",
      "for s in graph_with_cross_memory.stream(inputs_user1_1, config_user1_thread1, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "print(\"\\n--- User 1 tells the agent to save a memory ---\")\n",
      "inputs_user1_2 = {\"messages\": [(\"user\", \"Please remember that I like hiking.\")]}\n",
      "# Agent might use the save_memory tool here\n",
      "\n",
      "for s in graph_with_cross_memory.stream(inputs_user1_2, config_user1_thread1, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "print(\"\\n--- Conversation with Cross-Thread Memory (User 1, Thread 2) ---\")\n",
      "config_user1_thread2 = {\"configurable\": {\"thread_id\": \"user1-thread-2\", \"user_id\": \"user1\"}}\n",
      "inputs_user1_3 = {\"messages\": [(\"user\", \"Hi! Do you remember me?\")]}\n",
      "\n",
      "# The agent should recall the saved memory because the user_id is the same\n",
      "for s in graph_with_cross_memory.stream(inputs_user1_3, config_user1_thread2, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "print(\"\\n--- Conversation with Cross-Thread Memory (User 2, Thread 1) ---\")\n",
      "config_user2_thread1 = {\"configurable\": {\"thread_id\": \"user2-thread-1\", \"user_id\": \"user2\"}}\n",
      "inputs_user2_1 = {\"messages\": [(\"user\", \"Hey, I'm Bob. Have we met?\")]}\n",
      "\n",
      "# The agent should NOT recall Alice's memory because the user_id is different\n",
      "for s in graph_with_cross_memory.stream(inputs_user2_1, config_user2_thread1, stream_mode=\"values\"):\n",
      "    message = s[\"messages\"][-1]\n",
      "    if isinstance(message, tuple):\n",
      "        print(message)\n",
      "    else:\n",
      "        message.pretty_print()\n",
      "\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1.  **Import necessary classes:** We import `create_react_agent`, `MemorySaver`, and components for tools, models, and state (`TypedDict`, `Annotated`, `BaseMessage`, `add_messages`). For the cross-thread memory pattern, we also import `InMemoryStore`, `BaseStore`, `InjectedStore`, `RunnableConfig`, and `SystemMessage`.\n",
      "2.  **Define Tools and Model:** We define a sample tool (`check_weather`) and initialize a chat model (`ChatOpenAI`).\n",
      "3.  **Create Checkpointer:** We instantiate `MemorySaver()` [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py]. This object is passed to `create_react_agent` to enable thread-specific memory. The state of each `thread_id` is saved and loaded automatically.\n",
      "4.  **Create Agent:** We call `create_react_agent` with the model, tools, and the `checkpointer`. This builds the core agent graph.\n",
      "5.  **Invoke with Config:** When invoking the graph (`graph.stream` or `graph.invoke`), we pass a `config` dictionary with a `configurable` key containing a unique `thread_id`. This `thread_id` tells the checkpointer which conversation state to load and save, providing thread-level memory.\n",
      "6.  **Cross-Thread Memory Pattern:** The second part of the example demonstrates the pattern shown in the context for cross-thread memory.\n",
      "    *   We define a `save_memory` tool that takes `store: Annotated[BaseStore, InjectedStore()]` [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py]. This allows LangGraph to automatically inject the configured `store` object when the tool is called.\n",
      "    *   We define `prepare_model_inputs`, a function that takes the `state`, `config`, and the injected `store`. It retrieves memories from the store based on the `user_id` in the config and adds them as a system message to the LLM's input.\n",
      "    *   We create an `InMemoryStore()` [libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py].\n",
      "    *   We create a *new* agent using `create_react_agent`, passing the `save_memory` tool, the `prepare_model_inputs` function as the `prompt`, and the `InMemoryStore` as the `store`.\n",
      "    *   When invoking this second agent, we include both `thread_id` (for conversation state memory via checkpointer) and `user_id` (for cross-thread user memory via the store and custom prompt/tool) in the config.\n",
      "\n",
      "To achieve memory storage in a *database*, you would need to replace `MemorySaver()` with a database-backed checkpointer (like `SQLSaver` or `RedisSaver`, which are not detailed in the provided context) and replace `InMemoryStore()` with a database-backed store implementation (like `SQLStore` or `RedisStore`, also not detailed in the provided context). The pattern of using the `checkpointer` and `store` parameters in `create_react_agent` would remain the same.\n",
      "\n",
      "To build a *multi-agent* system, you would typically create multiple individual agents (potentially using `create_react_agent` for each) and then compose them within a larger `StateGraph` or `MessageGraph`, defining nodes for each agent and conditional edges to route between them based on the state. The provided context, however, does not include the code for this higher-level multi-agent orchestration.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run a test ---------------------------------------------------------\n",
    "# question = \"How to strore interactions using DatabaseSessionService (when i run adk web). Tell me the exact command i should run ?\"\n",
    "question = \"\"\"\n",
    "\n",
    "Create a multi-agent system for me, should be a chatbot that has memory inbuilt and storing the memory in DB \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "response = rag_graph.invoke({\"question\": question})\n",
    "\n",
    "print(\"🧠 Answer:\\n\", response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Answer:\n",
      " ## Memory Management in LangGraph\n",
      "### Overview\n",
      "LangGraph's memory management is a crucial component that enables the storage and retrieval of user queries, intermediate results, and other relevant data. This explanation will walk through the process of memory management, from receiving a user query to storing the memory.\n",
      "\n",
      "### Step 1: Receiving User Query\n",
      "When a user submits a query, it is processed by the LangGraph application. The query is then passed through the graph-based workflow, where it is analyzed and executed.\n",
      "\n",
      "### Step 2: Memory Store Configuration\n",
      "The memory store is configured using the `StoreConfig` class, which defines the settings for the built-in long-term memory store. The configuration includes options for semantic search, TTL (time-to-live) behavior, and other settings.\n",
      "\n",
      "```python\n",
      "[libs/cli/langgraph_cli/config.py]\n",
      "class StoreConfig(TypedDict, total=False):\n",
      "    index: Optional[IndexConfig]\n",
      "    ttl: Optional[TTLConfig]\n",
      "```\n",
      "\n",
      "### Step 3: Storing Data in Memory\n",
      "The `InMemoryStore` class is used to store data in memory. The `store_in_memory` function yields an instance of `InMemoryStore`, which is used to store and retrieve data.\n",
      "\n",
      "```python\n",
      "[libs/langgraph/tests/conftest.py]\n",
      "def store_in_memory():\n",
      "    yield InMemoryStore()\n",
      "```\n",
      "\n",
      "### Step 4: Saving Checkpoints\n",
      "The `MemorySaverNoPending` class is used to save checkpoints, which represent the state of the graph at a particular point in time. The `get_tuple` method is used to retrieve the checkpoint tuple, which contains the configuration, checkpoint, and metadata.\n",
      "\n",
      "```python\n",
      "[libs/langgraph/tests/memory_assert.py]\n",
      "class MemorySaverNoPending(InMemorySaver):\n",
      "    def get_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
      "        result = super().get_tuple(config)\n",
      "        if result:\n",
      "            return CheckpointTuple(result.config, result.checkpoint, result.metadata)\n",
      "        return result\n",
      "```\n",
      "\n",
      "### Step 5: Storing Data in PostgreSQL\n",
      "For persistent storage, LangGraph uses PostgreSQL as a backing store. The `PoolConfig` class is used to configure the connection pool settings for PostgreSQL connections.\n",
      "\n",
      "```python\n",
      "[libs/checkpoint-postgres/langgraph/store/postgres/base.py]\n",
      "class PoolConfig(TypedDict, total=False):\n",
      "    min_size: int\n",
      "    max_size: Optional[int]\n",
      "    kwargs: dict\n",
      "```\n",
      "\n",
      "### Step 6: Storing Items in the Store\n",
      "The `StorePut` class is used to store, update, or delete items in the store. The `namespace`, `key`, `value`, and `index` fields are used to identify and store the item.\n",
      "\n",
      "```python\n",
      "[libs/sdk-py/langgraph_sdk/auth/types.py]\n",
      "class StorePut(typing.TypedDict):\n",
      "    namespace: tuple[str, ...]\n",
      "    key: str\n",
      "    value: typing.Optional[dict[str, typing.Any]]\n",
      "    index: typing.Optional[typing.Union[typing.Literal[False], list[str]]]\n",
      "```\n",
      "\n",
      "### Example Use Case\n",
      "To demonstrate the memory management process, consider the following example:\n",
      "\n",
      "1. A user submits a query to the LangGraph application.\n",
      "2. The query is processed by the graph-based workflow and analyzed.\n",
      "3. The `InMemoryStore` is used to store intermediate results and other relevant data.\n",
      "4. The `MemorySaverNoPending` class is used to save checkpoints, representing the state of the graph.\n",
      "5. The data is stored in PostgreSQL using the `PoolConfig` class.\n",
      "6. The `StorePut` class is used to store the final result in the store.\n",
      "\n",
      "By following these steps, LangGraph's memory management system enables the efficient storage and retrieval of user queries, intermediate results, and other relevant data.\n",
      "\n",
      "### Advantages\n",
      "The LangGraph memory management system offers several advantages, including:\n",
      "\n",
      "*   **Modularity**: The memory store is designed to be modular, allowing for different storage implementations (e.g., in-memory, PostgreSQL).\n",
      "*   **Scalability**: The use of PostgreSQL as a backing store enables scalability and persistence.\n",
      "*   **Flexibility**: The `StoreConfig` class allows for customization of the memory store settings.\n",
      "\n",
      "### Trade-Offs\n",
      "While the LangGraph memory management system offers several advantages, there are also some trade-offs to consider:\n",
      "\n",
      "*   **Complexity**: The use of multiple storage implementations and configuration options can add complexity to the system.\n",
      "*   **Resource Utilization**: The use of PostgreSQL as a backing store can consume additional resources.\n",
      "\n",
      "By understanding the memory management process in LangGraph, developers can design and implement efficient and scalable graph-based workflows.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Run a test ---------------------------------------------------------\n",
    "# question = \"How to strore interactions using DatabaseSessionService (when i run adk web). Tell me the exact command i should run ?\"\n",
    "question = \"\"\"\n",
    "How does memory management works, explain right from question to storing the memory of the user queries etc. \n",
    "\"\"\"\n",
    "response = rag_graph.invoke({\"question\": question})\n",
    "\n",
    "print(\"🧠 Answer:\\n\", response[\"answer\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
